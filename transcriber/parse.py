import stanza
from stanza.pipeline.core import DownloadMethod
import uuid
import os
from dotenv import load_dotenv
from helper import *
from functools import partial

# load environment variables from .env file
load_dotenv()

# save environment variables as constants
OUTPUT_FILE_PATH = os.getenv("OUTPUT_FILE_PATH")
PARSE_OUTPUT_PATH = os.getenv("PARSE_OUTPUT_PATH")

# read transcription
transcriptions = read_csv(OUTPUT_FILE_PATH)

# A function to map the transcribed document into the format to save
def map_doc(doc):
    doc['title'] = doc['id']
    doc['id'] = uuid.uuid4()
    doc['raw'] = doc['text']
    doc.pop('text', None)
    return doc


docs = list(map(map_doc, transcriptions))
tokens = []
sentences = []

# A function to map token generated by the Stanza into the format to save
def map_token(sentence_id, token):
    token['index'] = token['id']
    token['id'] = uuid.uuid4()
    token['sentence_id'] = sentence_id
    return token

# *Not used* constituency parsing is not used in the report to analyze word order
# A function to convert the ParseTree class in Stanza to a dict so it can be saved as json string
def parse_tree_to_dict(tree):
    # leaf
    if len(tree.children) < 1:
        return {
            'label': tree.label,
        }
    # node, recursive
    return {
        'label': tree.label,
        'children': [parse_tree_to_dict(child) for child in tree.children]
    }

# initialize Stanza pipeline
nlp = stanza.Pipeline(lang='es', 
                      processors='tokenize,mwt,pos,lemma,depparse,constituency', 
                      model_dir='.cache/stanza',
                      download_method=DownloadMethod.REUSE_RESOURCES # so it can work offline
                      )

for raw_doc in docs:
    # parse document into sentences and tokens
    doc = nlp(raw_doc['raw'])
    for i, sentence in enumerate(doc.sentences):
        # assign unique id for sentence
        sentence_id = uuid.uuid4()
        # record sentence
        sentences.append({
            'id': sentence_id,
            'index': i,
            'doc_id': raw_doc['id'],
            'raw': sentence.text,
            'constituency' : str(parse_tree_to_dict(sentence.constituency)) # saved as json string
        })

        # generate a new function with the original map_token but replace sentence_id accordingly
        map_token_partial = partial(map_token, str(sentence_id))
        # map and record tokens
        tokens += map(map_token_partial, sentence.to_dict())

print("Saving...")

# create output folder if not exists
os.makedirs(PARSE_OUTPUT_PATH, exist_ok=True)

# save data
write_csv(os.path.join(PARSE_OUTPUT_PATH, "documents.csv"), docs)
write_csv(os.path.join(PARSE_OUTPUT_PATH, "sentences.csv"), sentences)
write_csv(os.path.join(PARSE_OUTPUT_PATH, "tokens.csv"), tokens)

print("Done!")

